{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "614195be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import emoji, re, time\n",
    "\n",
    "import boto3\n",
    "import pyarrow.parquet as pymysql\n",
    "from dotenv import load_dotenv\n",
    "from io import BytesIO\n",
    "import io\n",
    "from PIL import Image\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n",
      "12.4\n",
      "True\n",
      "NVIDIA H100 80GB HBM3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)           \n",
    "print(torch.version.cuda)          \n",
    "print(torch.cuda.is_available())   \n",
    "print(torch.cuda.get_device_name(0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "602433f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('/home/flexmatch_ftp/ftp/multimodal_categorizer/config/.env')\n",
    "\n",
    "aws_access_key = os.getenv(\"aws_accessKey\")\n",
    "aws_secret_key = os.getenv(\"aws_secretKey\")\n",
    "region_name='ap-northeast-2'\n",
    "\n",
    "# S3 í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
    "s3_client = boto3.client('s3', aws_access_key_id=aws_access_key, aws_secret_access_key=aws_secret_key, region_name=region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0642c6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path_df = pd.read_parquet(\"/home/flexmatch_ftp/ftp/multimodal_categorizer/all_user_image_data.parquet\")\n",
    "img_path = img_path_df['s3_path'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e6a9862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucket_name = 'flexmatch-data'\n",
    "# sample = img_path[0]\n",
    "# sample_img = load_image_from_s3(bucket_name, sample)\n",
    "# sample_img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ea9fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [41:10<00:00,  4.05it/s] \n"
     ]
    }
   ],
   "source": [
    "# ì´ë¯¸ì§€ë§Œ ë¡œë”© \n",
    "\n",
    "bucket_name = 'flexmatch-data'\n",
    "\n",
    "def load_image_from_s3(bucket, key):\n",
    "    obj = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    img = Image.open(io.BytesIO(obj['Body'].read()))\n",
    "    return img.convert(\"RGB\")\n",
    "\n",
    "images = []\n",
    "for key in tqdm(img_path[:10000]):  # ì˜ˆì‹œë¡œ 100ì¥ë§Œ\n",
    "    try:\n",
    "        img = load_image_from_s3(bucket_name, key)\n",
    "        images.append(img)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {key}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4d5f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_texts_for_clip(df):\n",
    "    combined_texts = [\n",
    "        f\"{acnt_sub_nm}. {intro}. {txt}\"\n",
    "        for acnt_sub_nm, intro, txt in zip(\n",
    "            df[\"acnt_sub_nm\"],\n",
    "            df[\"intro_txt\"],\n",
    "            df[\"media_text\"]\n",
    "        )\n",
    "    ]\n",
    "    return combined_texts\n",
    "\n",
    "combined_texts = combine_texts_for_clip(img_path_df)\n",
    "combined_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5202b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ì´ë¯¸ì§€ë§Œ ì²˜ë¦¬í•œë‹¤ê³  í–ˆì„ ë•Œ\n",
    "\n",
    "# model_name = \"openai/clip-vit-base-patch32\"\n",
    "# # ì´ë¯¸ì§€ resize/normalized ë° text tokení™” ë° tensor ë³€í™˜ -> CLIPì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜\n",
    "# processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "# # CLIP ì›ë³¸ ëª¨ë¸ -> ì´ë¯¸ì§€ ë° í…ìŠ¤íŠ¸ ì„ë² ë”© ë³€í™˜í•˜ëŠ” ìš©ë„\n",
    "# model = CLIPModel.from_pretrained(model_name)\n",
    "\n",
    "# inputs = processor(images=images, return_tensors=\"pt\", padding=True, use_fast=True)\n",
    "# with torch.no_grad():\n",
    "#     image_features = model.get_image_features(**inputs)\n",
    "\n",
    "# class CLIPMultimodalClassifier(nn.Module):\n",
    "#     def __init__(self, model_name=\"openai/clip-vit-base-patch32\", num_classes=5):\n",
    "#         super().__init__()\n",
    "#         self.clip = CLIPModel.from_pretrained(model_name)\n",
    "#         self.fc = nn.Linear(self.clip.visual_projection.out_features * 2, num_classes)\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "#     def forward(self, image, text):\n",
    "#         outputs = self.clip.get_image_features(image), self.clip.get_text_features(text)\n",
    "#         image_feat = outputs[0] / outputs[0].norm(p=2, dim=-1, keepdim=True)\n",
    "#         text_feat = outputs[1] / outputs[1].norm(p=2, dim=-1, keepdim=True)\n",
    "#         combined = torch.cat([image_feat, text_feat], dim=1)\n",
    "#         logits = self.fc(combined)\n",
    "#         return self.softmax(logits)\n",
    "\n",
    "# # ì˜ˆì‹œ ì…ë ¥\n",
    "# from transformers import CLIPProcessor\n",
    "# from PIL import Image\n",
    "\n",
    "# model = CLIPMultimodalClassifier(num_classes=3)\n",
    "# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# image = Image.open(\"example.jpg\")\n",
    "# text = [\"ì‹±ê·¸ëŸ¬ìš´ ë´„ íŒ¨ì…˜ ë¦¬ë·°\"]\n",
    "# inputs = processor(text=text, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# # forward\n",
    "# pred = model(inputs[\"pixel_values\"], inputs[\"input_ids\"])\n",
    "# print(pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a3c9fb",
   "metadata": {},
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88128905",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"flexmatch-data\"\n",
    "MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class S3StreamingDataset(Dataset):\n",
    "    def __init__(self, df, bucket):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.bucket = bucket\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    ## getitem ë§ˆë‹¤ s3.get_object() í˜¸ì¶œ\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        key = row[\"s3_path\"]\n",
    "\n",
    "        try:\n",
    "            # ë§¤ë²ˆ S3ì—ì„œ ì§ì ‘ ì½ê¸°\n",
    "            obj = s3_client.get_object(Bucket=self.bucket, Key=key)\n",
    "            img = Image.open(BytesIO(obj[\"Body\"].read())).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] S3 read failed for {key}: {e}\")\n",
    "            img = Image.new(\"RGB\", (224,224), (0,0,0))\n",
    "\n",
    "        # í…ìŠ¤íŠ¸ í•„ë“œ í•©ì¹˜ê¸°\n",
    "        text_parts = []\n",
    "        for col in [\"acnt_sub_nm\", \"intro_txt\", \"media_text\"]:\n",
    "            if col in row and pd.notna(row[col]):\n",
    "                text_parts.append(str(row[col]).strip())\n",
    "        combined_text = \". \".join(text_parts) if text_parts else \"\"\n",
    "\n",
    "        return {\"image\": img, \"text\": combined_text, \"s3_key\": key}\n",
    "\n",
    "## ì´ë¯¸ì§€+í…ìŠ¤íŠ¸ë¥¼ ë¬¶ì–´ ë°°ì¹˜ë¡œ ì „ë‹¬ -> ì´í›„ì— processorì— ë„£ì„ ìˆ˜ ìˆìŒ\n",
    "def collate_fn(batch):\n",
    "    images = [item[\"image\"] for item in batch]\n",
    "    texts = [item[\"text\"] for item in batch]\n",
    "    keys = [item[\"s3_key\"] for item in batch]\n",
    "    return {\"images\": images, \"texts\": texts, \"keys\": keys}\n",
    "\n",
    "## clip ëª¨ë¸/í”„ë¡œì„¸ì„œ ë¡œë“œ\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
    "model = CLIPModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "\n",
    "def extract_clip_embeddings(df):\n",
    "    dataset = S3StreamingDataset(df, BUCKET_NAME)\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    all_image_embeds, all_text_embeds, all_keys = [], [], []\n",
    "\n",
    "    for batch in tqdm(loader, desc=\"Extracting CLIP embeddings\"):\n",
    "        try:\n",
    "            inputs = processor(\n",
    "                text=batch[\"texts\"],\n",
    "                images=batch[\"images\"],\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "            ).to(DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                image_embeds = outputs.image_embeds\n",
    "                text_embeds = outputs.text_embeds\n",
    "\n",
    "            all_image_embeds.append(image_embeds.cpu())\n",
    "            all_text_embeds.append(text_embeds.cpu())\n",
    "            all_keys.extend(batch[\"keys\"])\n",
    "\n",
    "            # ë©”ëª¨ë¦¬ í•´ì œ\n",
    "            del inputs, outputs, image_embeds, text_embeds\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Batch failed: {e}\")\n",
    "            continue\n",
    "\n",
    "    image_tensor = torch.cat(all_image_embeds, dim=0)\n",
    "    text_tensor = torch.cat(all_text_embeds, dim=0)\n",
    "\n",
    "    return image_tensor, text_tensor, all_keys\n",
    "\n",
    "### ---ì‚¬ìš©---\n",
    "# dataset = S3StreamingDataset(img_path_df, BUCKET_NAME)\n",
    "# loader = DataLoader(\n",
    "#     dataset,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     shuffle=False,\n",
    "#     num_workers=NUM_WORKERS,\n",
    "#     collate_fn=collate_fn,\n",
    "# )\n",
    "\n",
    "# for i, batch in enumerate(tqdm(loader, desc=\"Streaming from S3\")):\n",
    "#     # ì—¬ê¸°ì— processor ì ìš© ê°€ëŠ¥\n",
    "#     print(f\"Batch {i} - {len(batch['images'])} images loaded.\")\n",
    "#     print(f\"Sample text: {batch['texts'][0][:100]}\")\n",
    "#     if i >= 2:  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 3ë°°ì¹˜ë§Œ\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715e8ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    image_embeds, text_embeds, keys = extract_clip_embeddings(img_path_df)\n",
    "\n",
    "    print(\"ì„ë² ë”© ì™„ë£Œ!\")\n",
    "    print(f\"ì´ë¯¸ì§€ ì„ë² ë”© í¬ê¸°: {image_embeds.shape}\")\n",
    "    print(f\"í…ìŠ¤íŠ¸ ì„ë² ë”© í¬ê¸°: {text_embeds.shape}\")\n",
    "\n",
    "    # ì˜ˆì‹œë¡œ ë¡œì»¬ì— ì €ì¥\n",
    "    torch.save({\n",
    "        \"keys\": keys,\n",
    "        \"image_embeds\": image_embeds,\n",
    "        \"text_embeds\": text_embeds\n",
    "    }, \"/home/flexmatch_ftp/ftp/multimodal_categorizer/clip_embeddings.pt\")\n",
    "\n",
    "    print(\"ğŸ’¾ clip_embeddings.pt ì €ì¥ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc30ff7c",
   "metadata": {},
   "source": [
    "##### multimodal classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2373e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_labels = ['IT', 'ê²Œì„', 'ê²°í˜¼/ì—°ì• ', 'êµìœ¡', 'ë‹¤ì´ì–´íŠ¸/ê±´ê°•ë³´ì¡°ì‹í’ˆ', 'ë§Œí™”/ì• ë‹ˆ/íˆ°', 'ë¬¸êµ¬/ì™„êµ¬', 'ë¯¸ìˆ /ë””ìì¸', 'ë°˜ë ¤ë™ë¬¼', 'ë² ì´ë¹„/í‚¤ì¦ˆ', 'ë·°í‹°', 'ë¸Œëœë“œê³µì‹ê³„ì •',\n",
    "                   'ì‚¬ì§„/ì˜ìƒ', 'ì…€ëŸ½', 'ìŠ¤í¬ì¸ ', 'ì‹œì‚¬', 'ì—”í„°í…Œì¸ë¨¼íŠ¸', 'ì—¬í–‰/ê´€ê´‘', 'ìœ ëª…ì¥ì†Œ/í•«í”Œ', 'ì¼ìƒ', 'ìë™ì°¨/ëª¨ë¹Œë¦¬í‹°', 'ì§¤/ë°ˆ', 'ì·¨ë¯¸', 'íŒ¨ì…˜', 'í‘¸ë“œ', 'í™ˆ/ë¦¬ë¹™']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c3d2cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(category_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a42e2caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    'IT': 0,\n",
    "    'ê²Œì„': 1,\n",
    "    'ê²°í˜¼/ì—°ì• ': 2,\n",
    "    'êµìœ¡': 3,\n",
    "    'ë‹¤ì´ì–´íŠ¸/ê±´ê°•ë³´ì¡°ì‹í’ˆ': 4,\n",
    "    'ë§Œí™”/ì• ë‹ˆ/íˆ°': 5,\n",
    "    'ë¬¸êµ¬/ì™„êµ¬': 6,\n",
    "    'ë¯¸ìˆ /ë””ìì¸': 7,\n",
    "    'ë°˜ë ¤ë™ë¬¼': 8,\n",
    "    'ë² ì´ë¹„/í‚¤ì¦ˆ': 9,\n",
    "    'ë·°í‹°': 10,\n",
    "    'ë¸Œëœë“œê³µì‹ê³„ì •': 11,\n",
    "    'ì‚¬ì§„/ì˜ìƒ': 12, # ì›ë˜ëŠ” ì‚¬ì§„/ì˜ìƒ\n",
    "    'ì…€ëŸ½': 13,\n",
    "    'ìŠ¤í¬ì¸ ': 14,\n",
    "    'ì‹œì‚¬': 15,\n",
    "    'ì—”í„°í…Œì¸ë¨¼íŠ¸': 16,\n",
    "    'ì—¬í–‰/ê´€ê´‘': 17,\n",
    "    'ìœ ëª…ì¥ì†Œ/í•«í”Œ': 18,\n",
    "    'ì¼ìƒ': 19,\n",
    "    'ìë™ì°¨/ëª¨ë¹Œë¦¬í‹°': 20,\n",
    "    'ì§¤/ë°ˆ': 21,\n",
    "    'ì·¨ë¯¸': 22,\n",
    "    'íŒ¨ì…˜': 23,\n",
    "    'í‘¸ë“œ': 24,\n",
    "    'í™ˆ/ë¦¬ë¹™': 25\n",
    "}\n",
    "\n",
    "img_path_df['label'] = img_path_df['main_category'].map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e24601d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path_df['label'] = img_path_df['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4f9fa2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"flexmatch-data\"\n",
    "# TEXT_COLS = [\"acnt_sub_nm_cleaned\",\"intro_txt_cleaned\",\"text\"]\n",
    "MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "BATCH_SIZE = 128       \n",
    "NUM_WORKERS = 8\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "CHECKPOINT_PATH = \"/home/flexmatch_ftp/ftp/multimodal_categorizer/clip_cls_ckpt.pt\"\n",
    "# ----------------------------\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_NAME, use_fast=False)  # or use_fast=True\n",
    "clip_backbone = CLIPModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "\n",
    "class S3StreamingDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "    \n",
    "    # ì „ì²´ ë°ì´í„° ê¸¸ì´ -> dataloaderê°€ ë°˜ë³µí•´ì•¼ í•  ì „ì²´ ë°ì´í„° ê¸¸ì´\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    # Dataloaderê°€ ë°°ì¹˜ë‹¨ìœ„ë¡œ ì¸ë±ìŠ¤ë¥¼ ë„˜ê²¨ì¤„ ë•Œë§ˆë‹¤ ê·¸ ë•Œ í•´ë‹¹ ìƒ˜í”Œë§Œ s3ì—ì„œ ì½ì–´ì˜´\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        key = row[\"s3_path\"]\n",
    "        try:\n",
    "            obj = s3_client.get_object(Bucket=BUCKET_NAME, Key=key)\n",
    "            img = Image.open(BytesIO(obj['Body'].read())).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            img = Image.new(\"RGB\",(224,224),(0,0,0))\n",
    "        \n",
    "        text_parts = []\n",
    "        for col in [\"acnt_sub_nm\", \"intro_txt\", \"media_text\"]:\n",
    "            if col in row and pd.notna(row[col]):\n",
    "                text_parts.append(str(row[col]).strip())\n",
    "        combined_text = \". \".join(text_parts) if text_parts else \"\"\n",
    "\n",
    "        # parts = [str(row[c]).strip() for c in TEXT_COLS if c in row and pd.notna(row[c])]\n",
    "        # text = \". \".join(parts) if parts else \"\"\n",
    "        \n",
    "        label = int(row['label'])\n",
    "        return {\"image\": img, \"text\": combined_text, \"label\": label}\n",
    "\n",
    "# ê° ìƒ˜í”Œë“¤ì„ í•˜ë‚˜ì˜ ë°°ì¹˜ë¡œ í•©ì¹˜ëŠ” ë°©ë²•ì„ ì»¤ìŠ¤í„°ë§ˆì´ì§•í•  ë•Œ ì‚¬ìš©\n",
    "# í•´ë‹¹ í•¨ìˆ˜ë¥¼ dataloader ì¸ìë¡œ ë„˜ê²¨ì£¼ë©´ ìë™ìœ¼ë¡œ batch ê°’ì„ ë„£ì–´ì¤Œ\n",
    "# batch_sizeì™€ ë™ì¼í•œ í¬ê¸°ë§Œí¼ì˜ ìƒ˜í”Œë¡œ ì´ë£¨ì–´ì§„ í•˜ë‚˜ì˜ ë°°ì¹˜ë¥¼ ìƒì„±í•˜ëŠ” ì—­í• \n",
    "# ê·¸ë¦¬ê³  ê·¸ processorë¡œ í…ì„œë³€í™˜ì„ ì§„í–‰\n",
    "def collate_fn(batch):\n",
    "    images = [b[\"image\"] for b in batch]\n",
    "    texts  = [b[\"text\"]  for b in batch]\n",
    "    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n",
    "    inputs = processor(text=texts, images=images, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    # move tensors later in loop to device\n",
    "    inputs[\"labels\"] = labels\n",
    "    return inputs\n",
    "\n",
    "# classifier wrapper\n",
    "class CLIPMulticlassModel(nn.Module):\n",
    "    def __init__(self, clip_model, num_classes, freeze_backbone=False):\n",
    "        super().__init__()\n",
    "        self.clip = clip_model\n",
    "        proj = self.clip.config.projection_dim\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(proj*2, proj),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(proj, num_classes)\n",
    "        )\n",
    "        if freeze_backbone:\n",
    "            for p in self.clip.parameters(): p.requires_grad = False\n",
    "    def forward(self, input_ids=None, attention_mask=None, pixel_values=None):\n",
    "        image_emb = self.clip.get_image_features(pixel_values=pixel_values)\n",
    "        text_emb  = self.clip.get_text_features(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        image_emb = image_emb / image_emb.norm(dim=-1, keepdim=True)\n",
    "        text_emb  = text_emb / text_emb.norm(dim=-1, keepdim=True)\n",
    "        combined = torch.cat([image_emb, text_emb], dim=1)\n",
    "        return self.classifier(combined)\n",
    "\n",
    "# class CLIPMulticlassModel(nn.Module):\n",
    "#     def __init__(self, clip_model, num_classes):\n",
    "#         super().__init__()\n",
    "#         self.clip = clip_model\n",
    "#         self.classifier = nn.Linear(self.clip.config.projection_dim * 2, num_classes)\n",
    "\n",
    "#     def forward(self, **inputs):\n",
    "#         device = next(self.parameters()).device  \n",
    "#         inputs = {k: v.to(device) if torch.is_tensor(v) else v for k, v in inputs.items()}\n",
    "\n",
    "#         image_emb = self.clip.get_image_features(**inputs)\n",
    "#         text_emb = self.clip.get_text_features(**inputs)\n",
    "\n",
    "#         image_emb = image_emb / image_emb.norm(dim=-1, keepdim=True)\n",
    "#         text_emb = text_emb / text_emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "#         combined = torch.cat([image_emb, text_emb], dim=1)\n",
    "#         return self.classifier(combined)\n",
    "\n",
    "\n",
    "# -------- main training --------\n",
    "def train(df):\n",
    "    num_classes = len(category_labels)\n",
    "    # batch_sizeë§Œí¼ ë°ì´í„° ìƒ˜í”Œë“¤ì„ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë³€ê²½\n",
    "    dataset = S3StreamingDataset(df)\n",
    "    # í•´ë‹¹ datasetì„ loaderë¡œ ë¶ˆëŸ¬ì™€ì„œ í…ì„œë¡œ ë³€í™˜ ì§„í–‰\n",
    "    # ë”°ë¼ì„œ dataloderëŠ” ìƒ˜í”Œì„ ë¬¶ê³  ë¬¶ì¸ ìƒ˜í”Œì„ ìµœì¢… ë°°ì¹˜ í…ì„œë¡œ ë³€í™˜í•˜ëŠ” ì—­í• ì„ ì§„í–‰\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True)\n",
    "    model = CLIPMulticlassModel(clip_backbone, num_classes).to(DEVICE)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    start = time.time()\n",
    "    global_step = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        model.train()\n",
    "        for batch in pbar:\n",
    "            # labelì„ ì œì™¸í•œ ëª¨ë“  ì…ë ¥ í…ì„œ(input_ids, pixel_values, attention_mask)ë¥¼ ì§€ì •ëœ gpu ì¥ì¹˜ë¡œ ì´ë™\n",
    "            inputs = {k: v.to(DEVICE) for k,v in batch.items() if k != \"labels\"}\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "            # ìˆœì „íŒŒ\n",
    "            logits = model(**inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            # ìµœì í™”\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "            # ì²˜ë¦¬ëœ ë°°ì¹˜ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸\n",
    "            global_step += 1\n",
    "            pbar.set_postfix({\"loss\": float(loss.detach().cpu())})\n",
    "            \n",
    "            # gpu ë©”ëª¨ë¦¬ ìµœì í™” ë° ê´€ë¦¬ë¥¼ ìœ„í•œ ë‹¨ê³„\n",
    "            del inputs, labels, logits, loss\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "        # checkpoint per epoch\n",
    "        torch.save({\"model\": model.state_dict(), \"opt\": opt.state_dict(), \"epoch\": epoch}, CHECKPOINT_PATH)\n",
    "    elapsed_hr = (time.time()-start)/3600\n",
    "    print(f\"Training done. time: {elapsed_hr:.2f} hr\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     train(img_path_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a0fbcd",
   "metadata": {},
   "source": [
    "train, test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fb51682f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 [TRAIN]:   0%|          | 0/774 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 [TRAIN]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 774/774 [52:30<00:00,  4.07s/it, loss=0.855] \n",
      "Epoch 1/3 [EVAL]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [17:37<00:00,  4.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: Train Loss=1.9079, Test Loss=0.9325, Accuracy=86.90%\n",
      "\n",
      "Training done. Total time: 1.17 hr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 [TRAIN]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 774/774 [50:20<00:00,  3.90s/it, loss=0.388]  \n",
      "Epoch 2/3 [EVAL]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [16:54<00:00,  3.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: Train Loss=0.5930, Test Loss=0.3910, Accuracy=95.85%\n",
      "\n",
      "Training done. Total time: 2.30 hr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 [TRAIN]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 774/774 [49:05<00:00,  3.81s/it, loss=0.171]  \n",
      "Epoch 3/3 [EVAL]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [16:27<00:00,  3.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: Train Loss=0.2585, Test Loss=0.2202, Accuracy=97.06%\n",
      "\n",
      "Training done. Total time: 3.40 hr\n"
     ]
    }
   ],
   "source": [
    "BUCKET_NAME = \"flexmatch-data\"\n",
    "MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "BATCH_SIZE = 128       \n",
    "NUM_WORKERS = 8\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "CHECKPOINT_PATH = \"/home/flexmatch_ftp/ftp/multimodal_categorizer/clip_cls_ckpt_train.pt\"\n",
    "\n",
    "def train(df):\n",
    "    train_df, test_df = train_test_split(df)\n",
    "    num_classes = len(category_labels)\n",
    "\n",
    "    train_dataset = S3StreamingDataset(train_df)\n",
    "    test_dataset = S3StreamingDataset(test_df)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "    model = CLIPMulticlassModel(clip_backbone, num_classes).to(DEVICE)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    start = time.time()\n",
    "    global_step = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [TRAIN]\")\n",
    "        for batch in pbar:\n",
    "            inputs = {k: v.to(DEVICE) for k,v in batch.items() if k != \"labels\"}\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "            logits = model(**inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            global_step += 1\n",
    "            pbar.set_postfix({\"loss\": float(loss.detach().cpu())})\n",
    "            \n",
    "            del inputs, labels, logits, loss\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # ëª¨ë¸ í‰ê°€ ì§„í–‰\n",
    "        model.eval()\n",
    "        test_loss, correct, total = 0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [EVAL]\"):\n",
    "                inputs = {k: v.to(DEVICE) for k, v in batch.items() if k != \"labels\"}\n",
    "                labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "                logits = model(**inputs)\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "                del inputs, labels, logits, loss, preds\n",
    "                gc.collect()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        acc = correct / total\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}: Train Loss={avg_train_loss:.4f}, Test Loss={avg_test_loss:.4f}, Accuracy={acc*100:.2f}%\\n\")\n",
    "\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict(),\n",
    "            \"opt\": opt.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"test_loss\": avg_test_loss,\n",
    "            \"accuracy\": acc\n",
    "        }, CHECKPOINT_PATH)\n",
    "\n",
    "        elapsed_hr = (time.time() - start) / 3600\n",
    "        print(f\"Training done. Total time: {elapsed_hr:.2f} hr\")\n",
    "\n",
    "        # checkpoint per epoch\n",
    "        torch.save({\"model\": model.state_dict(), \"opt\": opt.state_dict(), \"epoch\": epoch}, CHECKPOINT_PATH)\n",
    "\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train(img_path_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffbc70b",
   "metadata": {},
   "source": [
    "no labeling data inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "00878677",
   "metadata": {},
   "outputs": [],
   "source": [
    "class S3StreamingInferenceDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        key = row[\"s3_path\"]\n",
    "        try:\n",
    "            obj = s3_client.get_object(Bucket=BUCKET_NAME, Key=key)\n",
    "            img = Image.open(BytesIO(obj['Body'].read())).convert(\"RGB\")\n",
    "        except Exception:\n",
    "            img = Image.new(\"RGB\", (224,224), (0,0,0))\n",
    "        \n",
    "        text_parts = []\n",
    "        for col in [\"acnt_sub_nm\", \"intro_txt\", \"media_text\"]:\n",
    "            if col in row and pd.notna(row[col]):\n",
    "                text_parts.append(str(row[col]).strip())\n",
    "        combined_text = \". \".join(text_parts) if text_parts else \"\"\n",
    "\n",
    "        return {\"image\": img, \"text\": combined_text}\n",
    "\n",
    "\n",
    "def collate_fn_predict(batch):\n",
    "    texts = [b[\"text\"] for b in batch]\n",
    "    images = [b[\"image\"] for b in batch]\n",
    "    inputs = processor(\n",
    "        text=texts,\n",
    "        images=images,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    return {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "\n",
    "def predict(df, model):\n",
    "    model.eval()\n",
    "    ds = S3StreamingInferenceDataset(df)\n",
    "    loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn_predict)\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for inputs in tqdm(loader, desc=\"Predicting\"):\n",
    "            # ë””ë°”ì´ìŠ¤ ìƒíƒœ í™•ì¸ìš© ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€\n",
    "            for k, v in inputs.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    print(f\"{k}: {v.device}\")  # âœ… ì–´ë””ì— ìˆëŠ”ì§€ í™•ì¸\n",
    "                    inputs[k] = v.to(DEVICE, non_blocking=True)\n",
    "\n",
    "            model_device = next(model.parameters()).device\n",
    "            print(f\"Model device: {model_device}\")  # âœ… ëª¨ë¸ ë””ë°”ì´ìŠ¤ë„ í™•ì¸\n",
    "\n",
    "            logits = model(**inputs)\n",
    "            pred = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            preds.extend(pred)\n",
    "            break  # ì¼ë‹¨ í•œ batchë§Œ í™•ì¸\n",
    "\n",
    "    return preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "972e0e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2534/371550618.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_test_df.drop(['label'], axis=1, inplace=True)\n",
      "Predicting:   0%|          | 0/1 [00:22<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n",
      "Model device: cuda:0\n",
      "ì˜ˆì¸¡ ì™„ë£Œ!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipykernel_2534/371550618.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_test_df['pred_label'] = predictions\n",
      "/tmp/ipykernel_2534/371550618.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_test_df['pred_category'] = new_test_df['pred_label'].map({i: labels for i, labels in enumerate(category_labels)})\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    CHECKPOINT_PATH = \"/home/flexmatch_ftp/ftp/multimodal_categorizer/clip_cls_ckpt_train.pt\"\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "    \n",
    "    num_classes = len(category_labels)\n",
    "    clip_backbone = CLIPModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "    model = CLIPMulticlassModel(clip_backbone, num_classes)\n",
    "\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # testí•  label ì—†ëŠ” ìƒˆë¡œìš´ ë°ì´í„°\n",
    "    train_df, test_df = train_test_split(img_path_df, test_size=0.2, random_state=42)\n",
    "    new_test_df = test_df[:100]\n",
    "    new_test_df.drop(['label'], axis=1, inplace=True)\n",
    "    \n",
    "    predictions = predict(new_test_df, model)\n",
    "    new_test_df['pred_label'] = predictions\n",
    "    new_test_df['pred_category'] = new_test_df['pred_label'].map({i: labels for i, labels in enumerate(category_labels)})\n",
    "\n",
    "    new_test_df.to_csv(\"/home/flexmatch_ftp/ftp/multimodal_categorizer/inference_test.csv\")\n",
    "    print(\"ì˜ˆì¸¡ ì™„ë£Œ!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
